{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:15:18.244844Z",
     "start_time": "2019-04-17T08:15:18.233658Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T09:17:02.458587Z",
     "start_time": "2019-04-17T09:17:02.445672Z"
    }
   },
   "outputs": [],
   "source": [
    "headers = {'accept' : '*/*',\n",
    "          'user-agent' : 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0'}\n",
    "# text/html,application/xhtml+xml,application/\n",
    "base_url = 'http://doramatv.ru/list/country/japan?sortType=rate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T07:13:55.995063Z",
     "start_time": "2019-04-23T07:13:55.989075Z"
    }
   },
   "outputs": [],
   "source": [
    "def hh_parser(base_url, headers):\n",
    "    jobs = []\n",
    "    urls = []\n",
    "    urls.append(base_url)\n",
    "    session = requests.Session()\n",
    "    request = session.get(base_url, headers = headers)\n",
    "    if request.status_code == 200:\n",
    "        soup = bs(request.content, 'lxml')\n",
    "        try:\n",
    "            pagination = soup.find_all('a', attrs = {'data-qa' : 'pager-page'})\n",
    "            count = int(pagination[-1].text)\n",
    "            for i in range(count):\n",
    "                url = f'https://hh.ru/search/vacancy?text=Data+scientist&area=1&from=suggest_post&page={i}'\n",
    "                if url not in urls:\n",
    "                    urls.append(url)\n",
    "        except:\n",
    "            pass\n",
    "#     print(urls)    \n",
    "    for url in urls:\n",
    "        request = session.get(url, headers = headers)\n",
    "        soup = bs(request.content, 'lxml')\n",
    "        divs = soup.find_all('div', attrs = {'data-qa' : 'vacancy-serp__vacancy'})\n",
    "        for div in divs:\n",
    "            try:\n",
    "                title = div.find('a', attrs = {'data-qa' : 'vacancy-serp__vacancy-title'}).text\n",
    "                href = div.find('a', attrs = {'data-qa' : 'vacancy-serp__vacancy-title'})['href']\n",
    "                #\n",
    "                request2 = session.get(href, headers = headers)\n",
    "                soup2 = bs(request2.content, 'lxml')\n",
    "#                 divs2 = soup2.find_all('div', attrs = {'class' : 'vacancy-description'})\n",
    "                exp = soup2.find('span', attrs = {'data-qa' : 'vacancy-experience'}).text\n",
    "                descr = soup2.find('div', attrs = {'data-qa' : 'vacancy-description'}).text\n",
    "                skills = soup2.find_all('span', attrs = {'data-qa' : 'bloko-tag__text', 'class' : 'Bloko-TagList-Text'})\n",
    "                skills = [i.text for i in skills]\n",
    "                \n",
    "                #\n",
    "                company = div.find('a', attrs = {'data-qa' : 'vacancy-serp__vacancy-employer'}).text\n",
    "                try:\n",
    "                    salary = div.find('div', attrs = {'data-qa' : 'vacancy-serp__vacancy-compensation'}).text\n",
    "                except AttributeError:\n",
    "                    salary = 'Секретный секрет'\n",
    "                respon = div.find('div', attrs = {'data-qa' : 'vacancy-serp__vacancy_snippet_responsibility'}).text\n",
    "                require = div.find('div', attrs = {'data-qa' : 'vacancy-serp__vacancy_snippet_requirement'}).text\n",
    "                jobs.append({\n",
    "                    'title' : title,\n",
    "                    'href' : href,\n",
    "                    'company' : company,\n",
    "                    'salary' : salary,\n",
    "                    'exp' : exp,\n",
    "                    'skills' : skills,\n",
    "                    'respon' : respon,\n",
    "                    'require' : require,\n",
    "                    'descr' : descr\n",
    "                    \n",
    "                })\n",
    "#                 print(soup2)\n",
    "#                 print(exp, descr)\n",
    "            except:\n",
    "                pass\n",
    "#             break\n",
    "#             print(url)\n",
    "#     print(len(jobs))\n",
    "    return jobs\n",
    "        \n",
    "        \n",
    "def files_writer(jobs):\n",
    "    with open('hh_ds.csv', 'w') as file:\n",
    "        pen = csv.writer(file)\n",
    "        pen.writerow(('title', 'href', 'company', 'salary', 'exp', 'skills', 'respon', 'require', 'descr'))\n",
    "        for job in jobs:\n",
    "            pen.writerow((job['title'], job['href'], job['company'], job['salary'], job['exp'], job['skills'], job['respon'], job['require'], job['descr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T07:13:57.653371Z",
     "start_time": "2019-04-23T07:13:57.598781Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ac669f35721c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhh_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'base_url' is not defined"
     ]
    }
   ],
   "source": [
    "hh_parser(base_url, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T09:18:30.690326Z",
     "start_time": "2019-04-17T09:17:33.150216Z"
    }
   },
   "outputs": [],
   "source": [
    "jobs = hh_parser(base_url, headers)\n",
    "files_writer(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:29:56.924883Z",
     "start_time": "2019-04-17T08:29:56.901604Z"
    }
   },
   "outputs": [],
   "source": [
    "headers = {'accept' : '*/*',\n",
    "          'user-agent' : 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0'}\n",
    "# text/html,application/xhtml+xml,application/\n",
    "base_url = 'https://hh.ru/search/vacancy?text=Data+analyst&area=1&from=suggest_post&page=0'\n",
    "\n",
    "def hh_parser(base_url, headers):\n",
    "    jobs = []\n",
    "    urls = []\n",
    "    urls.append(base_url)\n",
    "    session = requests.Session()\n",
    "    request = session.get(base_url, headers = headers)\n",
    "    if request.status_code == 200:\n",
    "        soup = bs(request.content, 'lxml')\n",
    "        try:\n",
    "            pagination = soup.find_all('a', attrs = {'data-qa' : 'pager-page'})\n",
    "            count = int(pagination[-1].text)\n",
    "            for i in range(count):\n",
    "                url = f'https://hh.ru/search/vacancy?text=Data+scientist&area=1&from=suggest_post&page={i}'\n",
    "                if url not in urls:\n",
    "                    urls.append(url)\n",
    "        except:\n",
    "            pass\n",
    "#     print(urls)    \n",
    "    for url in urls:\n",
    "        request = session.get(url, headers = headers)\n",
    "        soup = bs(request.content, 'lxml')\n",
    "        divs = soup.find_all('div', attrs = {'data-qa' : 'vacancy-serp__vacancy'})\n",
    "        for div in divs:\n",
    "            try:\n",
    "                title = div.find('a', attrs = {'data-qa' : 'vacancy-serp__vacancy-title'}).text\n",
    "                href = div.find('a', attrs = {'data-qa' : 'vacancy-serp__vacancy-title'})['href']\n",
    "                \n",
    "                \n",
    "                company = div.find('a', attrs = {'data-qa' : 'vacancy-serp__vacancy-employer'}).text\n",
    "                try:\n",
    "                    salary = div.find('div', attrs = {'data-qa' : 'vacancy-serp__vacancy-compensation'}).text\n",
    "                except AttributeError:\n",
    "                    salary = 'Секретный секрет'\n",
    "                respon = div.find('div', attrs = {'data-qa' : 'vacancy-serp__vacancy_snippet_responsibility'}).text\n",
    "                require = div.find('div', attrs = {'data-qa' : 'vacancy-serp__vacancy_snippet_requirement'}).text\n",
    "                jobs.append({\n",
    "                    'title' : title,\n",
    "                    'href' : href,\n",
    "                    'company' : company,\n",
    "                    'salary' : salary,\n",
    "                    'respon' : respon,\n",
    "                    'require' : require\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "#             print(url)\n",
    "#     print(len(jobs))\n",
    "    return jobs\n",
    "        \n",
    "        \n",
    "def files_writer(jobs):\n",
    "    with open('hh_da.csv', 'w') as file:\n",
    "        pen = csv.writer(file)\n",
    "        pen.writerow(('title', 'href', 'company', 'salary', 'respon', 'require'))\n",
    "        for job in jobs:\n",
    "            pen.writerow((job['title'], job['href'], job['company'], job['salary'], job['respon'], job['require']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:30:16.314782Z",
     "start_time": "2019-04-17T08:30:03.468750Z"
    }
   },
   "outputs": [],
   "source": [
    "jobs = hh_parser(base_url, headers)\n",
    "files_writer(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:55:33.295101Z",
     "start_time": "2019-04-17T08:55:33.009029Z"
    }
   },
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "request2 = session.get('https://hh.ru/vacancy/30603091?query=Data%20scientist', headers = headers)\n",
    "soup2 = bs(request2.content, 'lxml')\n",
    "#                 divs2 = soup2.find_all('div', attrs = {'class' : 'vacancy-description'})\n",
    "exp = soup2.find('span', attrs = {'data-qa' : 'vacancy-experience'}).text\n",
    "descr = soup2.find('div', attrs = {'data-qa' : 'vacancy-description'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
